---
layout: post
title: Simultaneous Localization And Mapping
visible: 1
---

## Introduction
We want to solve the SLAM (Simultaneous Localization And Mapping) problem with detection of moving objects, but in this article, we are only going to consider a very simple detection of moving objects.

First, we are going to use a grid based representation of the map, for the static and dynamic enviroment. We are going to solve SLAM with the maximum likelihood method. Because we can't have the complete map, we are going to consider in the solutions only a local map that is updated incrementally. Then, we can calculate the location of the vehicle with a fast grid-based scanning method, and knowing the location we can update the local map. Once we have the map with the static information of the enviroment, we can calculate the dynamic information, by detecting the moving objects.

## Dataset
We are obtaining the dataset from <https://www.nuscenes.org/>. It contains LIDAR data, camera images, odometry information, ... But here, for now we are only going to use LIDAR data and odometry information of the vehicle.

### LIDAR
The first step is to get the LIDAR data for each timestep and clean up the data. We can get a list of detection points, which are 3D points in space. We need to first calibrate the data, which can be done easily because be can get the calibration quaternions from the same dataset and the translation vector. First, we rotate the points and then we translate them. The translation is easy, but to rotate a point given the quaternion numbers, we can build the rotation matrix and apply this rotation to the point. There is plenty of information on the internet on how to do this. For reference, this is the code to calculate the rotation matrix:

{% raw %}
```cpp
Quaternion(double* q) {
     double norm = 0;
     for (int i = 0; i < 4; i++) {
          q_[i] = q[i];
          norm += q[i] * q[i];
     }
     norm = sqrt(norm);
     for (int i = 0; i < 4; i++) {
          q_[i] /= norm;
     }

     double q_matrix[4][4] = {{q_[0], -q_[1], -q_[2], -q[3]},
                                        {q_[1],  q_[0], -q_[3],  q[2]},
                                        {q_[2],  q_[3],  q_[0], -q[1]},
                                        {q_[3], -q_[2],  q_[1],  q[0]}};
     double q_bar_matrix[4][4] = {{q_[0], -q_[1], -q_[2], -q[3]},
                                             {q_[1],  q_[0],  q_[3], -q[2]},
                                             {q_[2], -q_[3],  q_[0],  q[1]},
                                             {q_[3],  q_[2], -q_[1],  q[0]}};

     // Transpose q_bar_matrix.
     for (int i = 0; i < 4; i++) {
          for (int j = i + 1; j < 4; j++) {
               std::swap(q_bar_matrix[i][j], q_bar_matrix[j][i]);
          }
     } 

     // Dot product.
     for (int i = 1; i < 4; i++) {
          for (int j = 1; j < 4; j++) {
               rotation_matrix_[i - 1][j - 1] = 0.0;
               for (int k = 0; k < 4; k++) {
                    rotation_matrix_[i-1][j-1] += 
                         q_matrix[i][k] * q_bar_matrix[k][j];
               }
          }
     }
}
```
{% endraw %}

Once we have the LIDAR data calibrated, we need to remove unnecessary points, which consists of, the detection points of the own vehicle, the detection points of the ground plane, when the LIDAR beams hit the ground. And we will also remove the points that are too far away, because they have low quality. The first point is easy, we can remove all points that are inside some polygon, or easier, that are at a distance smaller than, for example, 1.5 meters from the detector, is also a good approximation.

Removing the points on the ground is a little more complex. We are going to use the RANSAC algorithm to find the equation of the ground plane, and then we can remove all points that are on the ground.

#### Ransac

The algorithms is:
1. Define number of iterations. I use a fixed number, but in the literature you can find the formula: <br />

     $$eps = 1 - forseeable\_support / point\_list.size$$ 

     $$N = round(\frac{log(1 - alpha) }{ log(1 - pow(1 - eps, 3))})$$
2. For each iteration, select 3 points randomly, and find the plane through those points.
3. Count the number of points inside the plane (the distance to the plane is less than a certain value)
4. Choose the plane which contains the maximum number of points, and has the less standra deviation.
5. Remove all points inside the best plane.

## Grid Based Bayesian Filtering

### Notation 

Z: Perception Measurements
U: Vehicle Motion Measurements

X: Vehicle State
M: Static Objects
O: Moving Obkects

### Updating the Occupancy Probability

Bayesian filters are used to estimate quantities with sensor data. Sensor data only contains partial information, and the measurements are corrupted with noise. We use probabilistic state estimation methods to recover state variables.

When the sensor is Gaussian and the dynamics model is linear with Gaussian noise then the sequential Bayesian filtering algorithm leads to the known kalman Filter.

For non-linear and non-Gaussian situations, it exists the Extended Kalman Filter. The Unscented Kalman Filter has a higher accuracy.

To handle highly nonlinear and non-Gaussian models, particle filters are more accurate than Kalman-based methods.

This equation describes how to update the occupancy probability of a grid map given sensor inputs.

$$ P(m|x_{1:t},z_{1:t}) = \left[ 1 + \frac{1 -P(m|x_{t},z_{t}) }{P(m|x_{t},z_{t})}\cdot\frac{P(m)}{1-P(m)}\cdot \frac{1 -P(m|x_{1:t-1},z_{1:t-1}) }{P(m|x_{1:t-1},z_{1:t-1})} \right]^{-1}$$

The probability $P$ $P(m \| x_{1:t-1},z_{1:t-1})$ is the occupancy probability estimated previously from measurements in the past. So we need to know $P(m \|x_{t},z_{t})$ and $P(m)$. $P(m)$ is the prior occupancy probability of the grid map cell, which in an unknown state has a value of 0.5. $P(m \|x_{t},z_{t})$ expreses the probability that a single cell m is occupied based on a single sensor measurement $z_t$ at a location $x_t$.

We are going to assume the occupancy probability at a cell m is going to be independent for all sensor measurements.

In the case of a laser sensor, we are going to consider a laser beam. If the laser beam gives a distance l, then we need to update all the grid cells inside the beam less than a distance l from the origin of the beam as free, and the grid cell at a distance l, as occupied.

$$
 P(m|x_t,z_t) = 
\left \{

\begin{array}{c l}	
     P_{free} & |f(x_t,n,k)|< z_t^n \\
     P_{occ} & |f(x_t,n,k)| = z_t^n \\
     P_{prior} & otherwise
\end{array}

\right \}

$$

We will use $P_{free}=0.2$, $P_{occ}=0.8$.

$$ f(c_{occ}, c_{free}) = \frac{exp(c_{occ}P_{occ} + c_{free}R_{free})}{1 + exp(c_{occ}P_{occ} + c_{free}R_{free})} $$

## Grid-based Scan Matching

To update the occupancy grid map over time given laser measurements, we need to know the vehicle location.  However, to obtain a good vehicle localization we can't use odometry sensors, as the error would be too large. We are going to use the incremental maximum likelihood SLAM method.

The idea is to mantain a series of maximum likelihood maps, M1, M2, ... along a series of maximum likelihood vehicle poses x1, x2, ...

$$ \left< x_t, M_t \right> = argmax_{x_t,M_t} \left[ P(z_t|x_t,M_t)P(x_t,M_t|u_t,x_{t-1}, M_{t-1}) \right] $$

Since usually the map Mt is uniquely determined by the pose xt, it suffices to search the space of poses xt.

$$ \left< x_t, M_t \right> = argmax_{x_t} \left[ P(z_t|x_t,M_t)P(x_t,M_t|u_t,x_{t-1}) \right] $$

Here, $P(z_t \|x_t,M_t)$ is the probability of observing the most recent measurement $z_t$ given the pose $x_t$ and the map $M_{t-1}$ constructed so far. The term $P(x_t,M_t \|u_t,x_{t-1})$ is the probability that the vehicle is at location $x_t$ given that previously it was at position $x_{t-1}$.

### Dynamics model

In a two-dimensional space, the pose can be described by the position in the coordinate frame, along its angular orientation.

$$
\left (

\begin{array}{c l}	
     x \\
     y \\
     \theta
\end{array}

\right )
$$

#### Probabilistic Kinematics

The model is

$$ P(x_t|u_t,x_{t-1}) $$

There are two types of models, velocity models and odometry models. The first model assumes that the motion data $u_t$ specifies the velocity commands. The second model assumes that we are provided with odometry information. Odometry models tend to be more accurate, because most robots don't execute velocity commands with the same level of accuracy that can be obtained by odometry, but this last one is only obtained after the fact, so it can't be used for motion planning. Odometry models are used for estimation and velocity models are used for probabilistic motion planning.

In this case we are going to use odometry models for this problem.

#### Algorithm motion_model_odometry

$$ \delta_{rot1} = atan2(\bar y'-\bar y, \bar x' - \bar x) - \bar \theta$$
$$ \delta_{trans} = \sqrt{ (\bar x - \bar x')^2 + (\bar y - \bar y')^2}$$
$$ \delta_{rot2} = \bar \theta' -  \bar \theta -\delta_{tor1} $$  

<br/>

$$ \hat \delta_{rot1} = atan2( y'- y,  x' -  x) - \bar \theta$$
$$ \hat \delta_{trans} = \sqrt{ ( x -  x')^2 + ( y -  y')^2}$$
$$ \hat \delta_{rot2} =  \theta' -   \theta -\delta_{tor1} $$

<br/>

$$ p1 = prob(\delta_{rot1} - \hat \delta_{rot1}, \alpha_1 \hat \delta_{rot1} + \alpha_2 \hat \delta _{trans})$$

$$ p2 = prob(\delta_{trans} - \hat \delta_{trans}, \alpha_3 \hat \delta_{trans} + \alpha_4 (\hat \delta _{rot1} + \hat \delta _{rot2}))$$

$$ p3 = prob(\delta_{rot2} - \hat \delta_{rot2}, \alpha_1 \hat \delta_{rot2} + \alpha_2 \hat \delta _{trans})$$

<br/>

$$ return \; p1 \cdot p2 \cdot p3$$

Where $\bar x_{t-1} = (\bar x, \bar y, \bar \theta)$ and $\bar x_{t} = (\bar x', \bar y', \bar \theta')$ are taken from the odometry.

The motion information is given by the pair

$$
u_t = \left (

\begin{array}{c l}	
     \bar x_{t-1} \\
     \bar x_t \\
\end{array}

\right )
$$


#### Algorithm sample_motion_model_odometry

$$ \delta_{rot1} = atan2(\bar y'-\bar y, \bar x' - \bar x) - \bar \theta$$
$$ \delta_{trans} = \sqrt{ (\bar x - \bar x')^2 + (\bar y - \bar y')^2}$$
$$ \delta_{rot2} = \bar \theta' -  \bar \theta -\delta_{tor1} $$  

<br/>

$$ \hat \delta_{rot1} = \delta_{rot1} - sample(\alpha_1\delta_{rot1} + \alpha_2\delta_{trans})$$
$$ \hat \delta_{trans} = \delta_{trans} - sample( \alpha_3 \hat \delta_{trans} + \alpha_4 (\hat \delta _{rot1} + \hat \delta _{rot2}))$$
$$ \hat \delta_{rot2} =  \delta_{rot2} - sample(\alpha_1 \hat \delta_{rot2} + \alpha_2 \hat \delta _{trans})) $$

<br/>

$$ x' = x + \hat \delta_{trans}\cos({\theta + \hat \delta_{rot1}})$$
$$ y' = y + \hat \delta_{trans}\sin({\theta + \hat \delta_{rot1}})$$
$$ \theta' = \theta + \hat \delta_{rot1} + \hat \delta_{rot2} $$

<br/>

$$return \; x_t = (x', y', \theta ')^T$$

The outcome of $\delta_{rot2} - \hat \delta_{rot2}$ must be truncated to lie in $[-\pi,\pi]$

The function $prob(a,b)$ implements an error distribution over a with zero mean and variance b.

The sample function is a random number generator according to a gaussian distribution with mean 0 and variance b. 
We show two possible implementations to get a normal function using a uniform distribution random number generator:

sample(b) = $\frac{b}{6}\sum_{i=1}^{12} rand(-1, 1)$

sample(b) = $b\cdot rand(-1, 1)\cdot rand(-1, 1)$

And prob(a, b) computes the probability of its arguments a under a zero-centered distribution with variance b.

* From Probabilistic Robotics, Sebastian Thrun, Wolfram Burgard, Dieter Fox, 2005

## Probability of Scan Measurement Given Vehicle Pose

Now, for each of the predictions of the localitation of the vehicle we need to calculate the probability of the scan measurement given that vehicle pose. The formula is:

$$ P(z_t|x_t,M_{t-1}) ~ \sum_{n=1}^{N}{P(M_{t-1}^{hit_t^n}) such that M_{t-1}{hit_t^n} is occupied}$$

And we will use the pose that maximizes the previous probability.

## Update Map

Once we know the location of the vehicle, we can update the values of the occupancy probabilities of the local map using the formulas we have defined earlier. But, what happens we the position is outside this local map? We need to create a new local map, that we will make so it overlaps slightly with the current one. With this, we will have a list of local map, that overlaps with each other, and will call this list the global map. We don't create a new local map when the localtion of the vehicle is outside the current maps, but when it is in the border.

For each iteration, we won't update only one map, but all of the local maps that contain the position of the vehicles. We do this to help mantain a coherent global map, and close the loop if we return to the same place. So for each iteration, we look over the list of local maps, and find every relevant one that we need to use to calculate the likelihood probability, and that we will need to update.


# Moving Object Detection

There are both static and dynamic objects that we will detect in the map with the previous approach. Moving objects in the mapping process will decrease the quality of the constructed map, so we need to detect these moving objects and differentiate them from the static objects.

The principal idea we are going to follow is that, if we detect an object in a space that was free previously, we are quite certain that it is a moving object. If we detect it in a occupied space, then it's probably a static object. On the other hand, if we detect an object in a previously unknown space, we can't say anything about it.

Another idea we can use is, if there is some space where there are frequently moving objects, we can use this information to improve our detection of dynamics objects. For this, we are going to define a new map, dynamic map, that is going to contain information about previously found dynamic objects. The dynamic map contains the number of observations that a dynamic object has been detected at that cell.

The first step is, given the dynamic map, static map, and measurements, decide the state of each grid, if it is free or occupied by a static or dynamic object:

$$
 state(z^n) = 
\left \{

\begin{array}{c l}	
     static & :M_{hit^n}=occupied \\
     dynamic & :M_{hit^n}= free \ or \ D_{hit^n} > \alpha \\
     unknown & :M_{hit^n}=unknown
\end{array}

\right \}

$$

The second state is, after determining the dynamic measurements, clustering the objects into separate groups. Two points are considered to belong to the same object if the distance is less than 0.3m.

Measurements detected to be from dynamic objects are not used to update the map in SLAM. For unknown measurements, we will first assume to be static objects until we find latter evidence.